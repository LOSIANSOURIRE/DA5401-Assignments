{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DA5401 A8: Ensemble Learning for Complex Regression Modeling\n**NAME:** Manish Nayak  \n**ROLL NO:** CE22B069\n","metadata":{}},{"cell_type":"markdown","source":"### Part A: Data Preprocessing and Baseline\n\n#### 1. Data Loading and Feature Engineering\n","metadata":{}},{"cell_type":"code","source":"pip install ucimlrepo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:44:27.623450Z","iopub.execute_input":"2025-11-10T07:44:27.623894Z","iopub.status.idle":"2025-11-10T07:44:33.903782Z","shell.execute_reply.started":"2025-11-10T07:44:27.623825Z","shell.execute_reply":"2025-11-10T07:44:33.902374Z"}},"outputs":[{"name":"stdout","text":"Collecting ucimlrepo\n  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.3)\nRequirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.10.5)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nDownloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\nInstalling collected packages: ucimlrepo\nSuccessfully installed ucimlrepo-0.0.7\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from ucimlrepo import fetch_ucirepo \n  \n# fetch dataset \nbike_sharing = fetch_ucirepo(id=275) \n  \n# data (as pandas dataframes) \nX = bike_sharing.data.features \ny = bike_sharing.data.targets \n  \n# metadata \nprint(bike_sharing.metadata) \n  \n# variable information \nprint(bike_sharing.variables) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:15:33.198214Z","iopub.execute_input":"2025-11-10T08:15:33.198543Z","iopub.status.idle":"2025-11-10T08:15:35.408921Z","shell.execute_reply.started":"2025-11-10T08:15:33.198521Z","shell.execute_reply":"2025-11-10T08:15:35.407612Z"}},"outputs":[{"name":"stdout","text":"{'uci_id': 275, 'name': 'Bike Sharing', 'repository_url': 'https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset', 'data_url': 'https://archive.ics.uci.edu/static/public/275/data.csv', 'abstract': 'This dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.', 'area': 'Social Science', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 17389, 'num_features': 13, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['cnt'], 'index_col': ['instant'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2013, 'last_updated': 'Sun Mar 10 2024', 'dataset_doi': '10.24432/C5W894', 'creators': ['Hadi Fanaee-T'], 'intro_paper': {'ID': 422, 'type': 'NATIVE', 'title': 'Event labeling combining ensemble detectors and background knowledge', 'authors': 'Hadi Fanaee-T, João Gama', 'venue': 'Progress in Artificial Intelligence', 'year': 2013, 'journal': None, 'DOI': '10.1007/s13748-013-0040-3', 'URL': 'https://www.semanticscholar.org/paper/bc42899f599d31a5d759f3e0a3ea8b52479d6423', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. \\r\\n\\r\\nApart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv\\r\\n\\t\\r\\n\\t- instant: record index\\r\\n\\t- dteday : date\\r\\n\\t- season : season (1:winter, 2:spring, 3:summer, 4:fall)\\r\\n\\t- yr : year (0: 2011, 1:2012)\\r\\n\\t- mnth : month ( 1 to 12)\\r\\n\\t- hr : hour (0 to 23)\\r\\n\\t- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\\r\\n\\t- weekday : day of the week\\r\\n\\t- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\\r\\n\\t+ weathersit : \\r\\n\\t\\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\\r\\n\\t\\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\\r\\n\\t\\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\\r\\n\\t\\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\\r\\n\\t- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\\r\\n\\t- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\\r\\n\\t- hum: Normalized humidity. The values are divided to 100 (max)\\r\\n\\t- windspeed: Normalized wind speed. The values are divided to 67 (max)\\r\\n\\t- casual: count of casual users\\r\\n\\t- registered: count of registered users\\r\\n\\t- cnt: count of total rental bikes including both casual and registered\\r\\n', 'citation': None}}\n          name     role         type demographic  \\\n0      instant       ID      Integer        None   \n1       dteday  Feature         Date        None   \n2       season  Feature  Categorical        None   \n3           yr  Feature  Categorical        None   \n4         mnth  Feature  Categorical        None   \n5           hr  Feature  Categorical        None   \n6      holiday  Feature       Binary        None   \n7      weekday  Feature  Categorical        None   \n8   workingday  Feature       Binary        None   \n9   weathersit  Feature  Categorical        None   \n10        temp  Feature   Continuous        None   \n11       atemp  Feature   Continuous        None   \n12         hum  Feature   Continuous        None   \n13   windspeed  Feature   Continuous        None   \n14      casual    Other      Integer        None   \n15  registered    Other      Integer        None   \n16         cnt   Target      Integer        None   \n\n                                          description units missing_values  \n0                                        record index  None             no  \n1                                                date  None             no  \n2                1:winter, 2:spring, 3:summer, 4:fall  None             no  \n3                             year (0: 2011, 1: 2012)  None             no  \n4                                     month (1 to 12)  None             no  \n5                                      hour (0 to 23)  None             no  \n6   weather day is holiday or not (extracted from ...  None             no  \n7                                     day of the week  None             no  \n8   if day is neither weekend nor holiday is 1, ot...  None             no  \n9   - 1: Clear, Few clouds, Partly cloudy, Partly ...  None             no  \n10  Normalized temperature in Celsius. The values ...     C             no  \n11  Normalized feeling temperature in Celsius. The...     C             no  \n12  Normalized humidity. The values are divided to...  None             no  \n13  Normalized wind speed. The values are divided ...  None             no  \n14                              count of casual users  None             no  \n15                          count of registered users  None             no  \n16  count of total rental bikes including both cas...  None             no  \n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"X.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:15:35.410750Z","iopub.execute_input":"2025-11-10T08:15:35.411124Z","iopub.status.idle":"2025-11-10T08:15:35.418644Z","shell.execute_reply.started":"2025-11-10T08:15:35.411100Z","shell.execute_reply":"2025-11-10T08:15:35.417454Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Index(['dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed'],\n      dtype='object')"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"X.drop(['dteday'],axis=1 , inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:15:37.944850Z","iopub.execute_input":"2025-11-10T08:15:37.945534Z","iopub.status.idle":"2025-11-10T08:15:37.953606Z","shell.execute_reply.started":"2025-11-10T08:15:37.945495Z","shell.execute_reply":"2025-11-10T08:15:37.952425Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/553028907.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X.drop(['dteday'],axis=1 , inplace=True)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"X.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:15:44.810622Z","iopub.execute_input":"2025-11-10T08:15:44.811001Z","iopub.status.idle":"2025-11-10T08:15:44.819031Z","shell.execute_reply.started":"2025-11-10T08:15:44.810978Z","shell.execute_reply":"2025-11-10T08:15:44.817586Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"Index(['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday',\n       'weathersit', 'temp', 'atemp', 'hum', 'windspeed'],\n      dtype='object')"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"#### Irrelevant columns were already not present in the dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Load the dataset\ntry:\n    df = pd.concat([bike_sharing.data.features, bike_sharing.data.targets], axis=1)\nexcept FileNotFoundError:\n    print(\"hour.csv not found. Please ensure the dataset is in the correct directory.\")\n\n# One-Hot Encode categorical features\ncategorical_features = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\ndf = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n\n# Display the first few rows of the preprocessed data\nprint(\"Preprocessed Data Head:\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:54:34.638690Z","iopub.execute_input":"2025-11-10T07:54:34.639045Z","iopub.status.idle":"2025-11-10T07:54:34.663088Z","shell.execute_reply.started":"2025-11-10T07:54:34.639022Z","shell.execute_reply":"2025-11-10T07:54:34.662256Z"}},"outputs":[{"name":"stdout","text":"Preprocessed Data Head:\n   yr  holiday  workingday  temp   atemp   hum  windspeed  cnt  season_2  \\\n0   0        0           0  0.24  0.2879  0.81        0.0   16     False   \n1   0        0           0  0.22  0.2727  0.80        0.0   40     False   \n2   0        0           0  0.22  0.2727  0.80        0.0   32     False   \n3   0        0           0  0.24  0.2879  0.75        0.0   13     False   \n4   0        0           0  0.24  0.2879  0.75        0.0    1     False   \n\n   season_3  ...  hr_20  hr_21  hr_22  hr_23  weekday_1  weekday_2  weekday_3  \\\n0     False  ...  False  False  False  False      False      False      False   \n1     False  ...  False  False  False  False      False      False      False   \n2     False  ...  False  False  False  False      False      False      False   \n3     False  ...  False  False  False  False      False      False      False   \n4     False  ...  False  False  False  False      False      False      False   \n\n   weekday_4  weekday_5  weekday_6  \n0      False      False       True  \n1      False      False       True  \n2      False      False       True  \n3      False      False       True  \n4      False      False       True  \n\n[5 rows x 54 columns]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:53:20.843376Z","iopub.execute_input":"2025-11-10T07:53:20.843723Z","iopub.status.idle":"2025-11-10T07:53:20.873271Z","shell.execute_reply.started":"2025-11-10T07:53:20.843698Z","shell.execute_reply":"2025-11-10T07:53:20.871760Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"       yr  holiday  workingday  temp   atemp   hum  windspeed  cnt  season_2  \\\n0       0        0           0  0.24  0.2879  0.81     0.0000   16     False   \n1       0        0           0  0.22  0.2727  0.80     0.0000   40     False   \n2       0        0           0  0.22  0.2727  0.80     0.0000   32     False   \n3       0        0           0  0.24  0.2879  0.75     0.0000   13     False   \n4       0        0           0  0.24  0.2879  0.75     0.0000    1     False   \n...    ..      ...         ...   ...     ...   ...        ...  ...       ...   \n17374   1        0           1  0.26  0.2576  0.60     0.1642  119     False   \n17375   1        0           1  0.26  0.2576  0.60     0.1642   89     False   \n17376   1        0           1  0.26  0.2576  0.60     0.1642   90     False   \n17377   1        0           1  0.26  0.2727  0.56     0.1343   61     False   \n17378   1        0           1  0.26  0.2727  0.65     0.1343   49     False   \n\n       season_3  ...  hr_20  hr_21  hr_22  hr_23  weekday_1  weekday_2  \\\n0         False  ...  False  False  False  False      False      False   \n1         False  ...  False  False  False  False      False      False   \n2         False  ...  False  False  False  False      False      False   \n3         False  ...  False  False  False  False      False      False   \n4         False  ...  False  False  False  False      False      False   \n...         ...  ...    ...    ...    ...    ...        ...        ...   \n17374     False  ...  False  False  False  False       True      False   \n17375     False  ...   True  False  False  False       True      False   \n17376     False  ...  False   True  False  False       True      False   \n17377     False  ...  False  False   True  False       True      False   \n17378     False  ...  False  False  False   True       True      False   \n\n       weekday_3  weekday_4  weekday_5  weekday_6  \n0          False      False      False       True  \n1          False      False      False       True  \n2          False      False      False       True  \n3          False      False      False       True  \n4          False      False      False       True  \n...          ...        ...        ...        ...  \n17374      False      False      False      False  \n17375      False      False      False      False  \n17376      False      False      False      False  \n17377      False      False      False      False  \n17378      False      False      False      False  \n\n[17379 rows x 54 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>yr</th>\n      <th>holiday</th>\n      <th>workingday</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>cnt</th>\n      <th>season_2</th>\n      <th>season_3</th>\n      <th>...</th>\n      <th>hr_20</th>\n      <th>hr_21</th>\n      <th>hr_22</th>\n      <th>hr_23</th>\n      <th>weekday_1</th>\n      <th>weekday_2</th>\n      <th>weekday_3</th>\n      <th>weekday_4</th>\n      <th>weekday_5</th>\n      <th>weekday_6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.81</td>\n      <td>0.0000</td>\n      <td>16</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0000</td>\n      <td>40</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0000</td>\n      <td>32</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0000</td>\n      <td>13</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0000</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17374</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.26</td>\n      <td>0.2576</td>\n      <td>0.60</td>\n      <td>0.1642</td>\n      <td>119</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17375</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.26</td>\n      <td>0.2576</td>\n      <td>0.60</td>\n      <td>0.1642</td>\n      <td>89</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17376</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.26</td>\n      <td>0.2576</td>\n      <td>0.60</td>\n      <td>0.1642</td>\n      <td>90</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17377</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.26</td>\n      <td>0.2727</td>\n      <td>0.56</td>\n      <td>0.1343</td>\n      <td>61</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17378</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.26</td>\n      <td>0.2727</td>\n      <td>0.65</td>\n      <td>0.1343</td>\n      <td>49</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>17379 rows × 54 columns</p>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"\n#### 2. Train/Test Split\n","metadata":{}},{"cell_type":"code","source":"# Define features (X) and target (y)\nX = df.drop('cnt', axis=1)\ny = df['cnt']\n\n# Split the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"\\nTraining set size: {X_train.shape[0]} samples\")\nprint(f\"Test set size: {X_test.shape[0]} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:55:32.587239Z","iopub.execute_input":"2025-11-10T07:55:32.587604Z","iopub.status.idle":"2025-11-10T07:55:32.602722Z","shell.execute_reply.started":"2025-11-10T07:55:32.587579Z","shell.execute_reply":"2025-11-10T07:55:32.601402Z"}},"outputs":[{"name":"stdout","text":"\nTraining set size: 13903 samples\nTest set size: 3476 samples\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"#### 3. Baseline Model (Single Regressor)\n","metadata":{}},{"cell_type":"code","source":"# Initialize the models\ndecision_tree = DecisionTreeRegressor(max_depth=6, random_state=42)\nlinear_regression = LinearRegression()\n\n# Train the models\ndecision_tree.fit(X_train, y_train)\nlinear_regression.fit(X_train, y_train)\n\n# Make predictions on the test set\ndt_predictions = decision_tree.predict(X_test)\nlr_predictions = linear_regression.predict(X_test)\n\n# Calculate RMSE for both models\ndt_rmse = np.sqrt(mean_squared_error(y_test, dt_predictions))\nlr_rmse = np.sqrt(mean_squared_error(y_test, lr_predictions))\n\nprint(f\"\\nDecision Tree RMSE: {dt_rmse:.4f}\")\nprint(f\"Linear Regression RMSE: {lr_rmse:.4f}\")\n\n# Determine the baseline model\nif dt_rmse < lr_rmse:\n    baseline_rmse = dt_rmse\n    baseline_model = \"Decision Tree\"\nelse:\n    baseline_rmse = lr_rmse\n    baseline_model = \"Linear Regression\"\n\nprint(f\"\\nBaseline Model: {baseline_model} with an RMSE of {baseline_rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T07:56:35.989183Z","iopub.execute_input":"2025-11-10T07:56:35.989642Z","iopub.status.idle":"2025-11-10T07:56:36.145180Z","shell.execute_reply.started":"2025-11-10T07:56:35.989616Z","shell.execute_reply":"2025-11-10T07:56:36.144065Z"}},"outputs":[{"name":"stdout","text":"\nDecision Tree RMSE: 118.4555\nLinear Regression RMSE: 100.4449\n\nBaseline Model: Linear Regression with an RMSE of 100.4449\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"Linear Regression worked better than Decision Tree throughout the range of 50 to 2000 estimators","metadata":{}},{"cell_type":"markdown","source":"### Part B: Ensemble Techniques for Bias and Variance Reduction\n\n#### 1. Bagging (Variance Reduction)\n\n**Hypothesis:** Bagging (Bootstrap Aggregating) is an ensemble technique that primarily aims to reduce the variance of a model.\n","metadata":{}},{"cell_type":"code","source":"# --- 1. Bagging (Variance Reduction) ---\n\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# Initialize the base estimator (the same single Decision Tree from our baseline)\nbase_dt = DecisionTreeRegressor(max_depth=6, random_state=42)\n\n# Initialize the Bagging Regressor\n# n_estimators is the number of base models to train\nbagging_reg = BaggingRegressor(\n    base_estimator=base_dt,\n    n_estimators=200,  # Using 100 estimators\n    random_state=42,\n    n_jobs=-1  # Use all available CPU cores\n)\n\n# Train the Bagging model\nprint(\"Training the Bagging Regressor...\")\nbagging_reg.fit(X_train, y_train)\nprint(\"Training complete.\")\n\n# Make predictions on the test set\nbagging_predictions = bagging_reg.predict(X_test)\n\n# Calculate and report the RMSE\nbagging_rmse = np.sqrt(mean_squared_error(y_test, bagging_predictions))\n\nprint(\"\\n--- Bagging Results ---\")\nprint(f\"Single Decision Tree RMSE (from Part A): 118.4555\")\nprint(f\"Bagging Regressor RMSE: {bagging_rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:02:21.596845Z","iopub.execute_input":"2025-11-10T08:02:21.597251Z","iopub.status.idle":"2025-11-10T08:02:24.667262Z","shell.execute_reply.started":"2025-11-10T08:02:21.597226Z","shell.execute_reply":"2025-11-10T08:02:24.666273Z"}},"outputs":[{"name":"stdout","text":"Training the Bagging Regressor...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training complete.\n\n--- Bagging Results ---\nSingle Decision Tree RMSE (from Part A): 118.4555\nBagging Regressor RMSE: 112.2620\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"### Bagging Results and Discussion\n\n**Calculated RMSE:**\n\n*   Single Decision Tree RMSE (from Part A): 118.4555\n*   **Bagging Regressor RMSE: 112.2620**\n\n**Discussion:**\n\nThe results clearly demonstrate that the bagging technique was effective in improving the model's performance. The Bagging Regressor achieved an RMSE of **112.2620**, which is a reduction of **6.1935** compared to the single Decision Tree's RMSE of 118.4555.\n\nThis supports the hypothesis that bagging primarily reduces variance. Here's the reasoning:\n\n1.  **High Variance of Single Trees:** A single Decision Tree is known to be a \"high variance\" estimator. This means that if you were to train it on slightly different subsets of the data, the resulting tree structures could be quite different, leading to inconsistent predictions. This instability is a classic sign of high variance.\n\n2.  **The Power of Averaging:** Bagging (Bootstrap Aggregating) mitigates this problem by creating many independent decision trees (100 in our case) on different random samples of the training data. While each individual tree might still have high variance and overfit its particular sample, their errors are diverse. By averaging the predictions of all these different trees, the individual errors tend to cancel each other out.\n\n3.  **Smoother, More Stable Predictions:** This averaging process results in a \"smoother\" and more stable final prediction. The bagged model is less sensitive to the specific noise and fluctuations in the training data, making it a more generalized and robust model. The reduction in RMSE from 118.4555 to 112.2620 is direct evidence of this improved generalization and, therefore, a successful reduction in model variance.","metadata":{}},{"cell_type":"markdown","source":"\n#### 2. Boosting (Bias Reduction)\n\n**Hypothesis:** Boosting is an ensemble technique that primarily aims to reduce a model's bias.","metadata":{}},{"cell_type":"code","source":"# --- 2. Boosting (Bias Reduction) ---\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Initialize the Gradient Boosting Regressor\n# We'll use some common hyperparameters to start\ngrad_boost_reg = GradientBoostingRegressor(\n    n_estimators=150,      # Number of sequential trees to build\n    learning_rate=0.1,     # How much each tree contributes to the final outcome\n    max_depth=5,           # Maximum depth of the individual trees\n    random_state=42\n)\n\n# Train the Gradient Boosting model\nprint(\"\\nTraining the Gradient Boosting Regressor...\")\ngrad_boost_reg.fit(X_train, y_train)\nprint(\"Training complete.\")\n\n# Make predictions on the test set\nboosting_predictions = grad_boost_reg.predict(X_test)\n\n# Calculate and report the RMSE\nboosting_rmse = np.sqrt(mean_squared_error(y_test, boosting_predictions))\n\nprint(\"\\n--- Boosting Results ---\")\nprint(f\"Baseline Linear Regression RMSE: 100.4449\")\nprint(f\"Bagging Regressor RMSE: {bagging_rmse:.4f}\")\nprint(f\"Gradient Boosting Regressor RMSE: {boosting_rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:04:44.729865Z","iopub.execute_input":"2025-11-10T08:04:44.730287Z","iopub.status.idle":"2025-11-10T08:04:50.883823Z","shell.execute_reply.started":"2025-11-10T08:04:44.730262Z","shell.execute_reply":"2025-11-10T08:04:50.882011Z"}},"outputs":[{"name":"stdout","text":"\nTraining the Gradient Boosting Regressor...\nTraining complete.\n\n--- Boosting Results ---\nBaseline Linear Regression RMSE: 100.4449\nBagging Regressor RMSE: 112.2620\nGradient Boosting Regressor RMSE: 54.1999\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"\n### Boosting Results and Discussion\n\n**Calculated RMSE:**\n\n*   Baseline Linear Regression RMSE: 100.4449\n*   Bagging Regressor RMSE: 112.2620\n*   **Gradient Boosting Regressor RMSE: 54.1999**\n\n**Discussion:**\n\nThe results show that the Gradient Boosting Regressor achieved a significantly better outcome than all previous models. With an RMSE of **54.1999**, it has nearly **halved the prediction error** of our best single model, the Linear Regression baseline (100.4449), and is substantially more accurate than the Bagging Regressor (112.2620).\n\nThis dramatic improvement strongly supports the hypothesis that **boosting effectively reduces bias**. Here's the reasoning:\n\n1.  **Sequential Error Correction:** Unlike bagging, which builds independent models in parallel, boosting is a sequential process. It trains the first weak learner (a shallow decision tree) on the data. The second learner is then trained not on the original data, but on the *errors* (residuals) of the first. Each subsequent learner is specifically built to correct the mistakes made by the ensemble of all preceding learners.\n\n2.  **Focusing on Weaknesses:** This sequential method forces the overall model to concentrate on the data points it finds most difficult to predict. By iteratively fixing its own weaknesses, the model's ability to capture the true underlying patterns in the data improves with each step. This fundamental inability to capture the true relationship is the model's **bias**.\n\n3.  **Evidence of Bias Reduction:** The massive drop in RMSE is the key piece of evidence. The baseline models, with errors over 100, had a relatively high bias; they were fundamentally unable to map the complex relationships between weather, time, and bike rentals. The Gradient Boosting model, by relentlessly correcting its errors, created a much more accurate and complex function that better represents the ground truth, thereby drastically reducing this bias and achieving a much lower overall error.\n\nIn conclusion, the superior performance of the Gradient Boosting Regressor is a clear demonstration of its strength in bias reduction, allowing it to build a far more accurate and powerful predictive model than either the single baseline models or the variance-reducing bagging ensemble.","metadata":{}},{"cell_type":"markdown","source":"\n### Part C: Stacking for Optimal Performance\n\n#### 1. Stacking Implementation\n\n**Principle of Stacking:**\n\nStacking, or Stacked Generalization, is an advanced ensemble method that combines multiple different regression (or classification) models to produce a final, improved prediction. It operates on a two-level structure:\n\n*   **Level-0 (Base Learners):** This level consists of several different models (e.g., KNN, Bagging, Boosting) that are all trained independently on the full training dataset. Their job is to learn the underlying patterns in the data from different perspectives. Diverse models are chosen intentionally because they will likely make different kinds of errors, which the next level can learn from.\n\n*   **Level-1 (Meta-Learner):** Instead of simply averaging the predictions of the base learners (like in bagging), Stacking trains a new model, the Meta-Learner, to make the final prediction. The training data for this Meta-Learner is not the original feature set. Instead, it is the set of predictions made by all the Level-0 Base Learners.\n\n**How the Meta-Learner Learns:**\n\nThe key insight of stacking is that the Meta-Learner learns the optimal way to combine the predictions from the base models. It effectively learns the strengths and weaknesses of each base learner. For example, it might learn that the Gradient Boosting model is very reliable in most cases but tends to over-predict during holidays, while the KNN model performs better in those specific situations. By training on the outputs of the base models, the Meta-Learner figures out what weights or combination rules to apply to their predictions to generate the most accurate final output. It essentially learns a sophisticated, data-driven way to \"trust\" each base model under different conditions.","metadata":{}},{"cell_type":"code","source":"\n# --- 1. Stacking Implementation ---\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n\n# Define the Base Learners (Level-0)\n# We will use the already trained Bagging and Boosting models.\n# To ensure diversity, we add a K-Nearest Neighbors model.\nbase_learners = [\n    ('knn', KNeighborsRegressor(n_neighbors=10)),\n    ('bagging', bagging_reg),  # Using the trained model from Part B\n    ('boosting', grad_boost_reg) # Using the trained model from Part B\n]\n\n# Define the Meta-Learner (Level-1)\n# Ridge regression is a good choice as it's a simple, regularized linear model.\nmeta_learner = Ridge(alpha=1.0)\n\n# Initialize the Stacking Regressor\n# The 'passthrough=True' argument allows the meta-learner to see both the\n# base model predictions AND the original features, which can sometimes improve performance.\n# cv=5 means 5-fold cross-validation will be used to generate the predictions for the meta-learner.\nstacking_reg = StackingRegressor(\n    estimators=base_learners,\n    final_estimator=meta_learner,\n    passthrough=True,\n    cv=5,\n    n_jobs=-1\n)\n\n# Train the Stacking model\n# This will take longer as it involves cross-validation and training multiple models.\nprint(\"Training the Stacking Regressor...\")\nstacking_reg.fit(X_train, y_train)\nprint(\"Training complete.\")\n\n# --- 2. Final Evaluation ---\n\n# Make predictions on the test set\nstacking_predictions = stacking_reg.predict(X_test)\n\n# Calculate and report the RMSE\nstacking_rmse = np.sqrt(mean_squared_error(y_test, stacking_predictions))\n\nprint(\"\\n--- Stacking Results ---\")\n# Using the final values from your previous runs\nprint(f\"Gradient Boosting Regressor RMSE: 54.1999\")\nprint(f\"Stacking Regressor RMSE: {stacking_rmse:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:08:53.022698Z","iopub.execute_input":"2025-11-10T08:08:53.024697Z","iopub.status.idle":"2025-11-10T08:09:26.295972Z","shell.execute_reply.started":"2025-11-10T08:08:53.024651Z","shell.execute_reply":"2025-11-10T08:09:26.294928Z"}},"outputs":[{"name":"stdout","text":"Training the Stacking Regressor...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training complete.\n\n--- Stacking Results ---\nGradient Boosting Regressor RMSE: 54.1999\nStacking Regressor RMSE: 49.4370\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"### Part D: Final Analysis\n\n#### 1. Comparative Table\n\n\n| Model | Technique | RMSE | Performance vs. Baseline |\n| :--- | :--- | :--- | :--- |\n| **Linear Regression** | **Baseline Single Model** | **100.4449** | **Baseline** |\n| Bagging Regressor | Bagging (Variance Reduction) | 112.2620 | -11.77% |\n| Gradient Boosting Regressor| Boosting (Bias Reduction) | 54.1999 | +46.04% |\n| **Stacking Regressor** | **Stacking (Optimal Combination)**| **49.4370** | **+50.78%** |\n\n*Performance vs. Baseline is calculated as `(1 - (Model RMSE / Baseline RMSE)) * 100`. A positive percentage indicates improvement.*\n\n---\n\n#### 2. Conclusion\n\n**Best-Performing Model:**\n\nBased on the empirical results, the **Stacking Regressor is unequivocally the best-performing model**, achieving the lowest RMSE of **49.4370**. It reduced the prediction error by over 50% compared to the initial baseline, demonstrating a substantial improvement in forecasting accuracy.\n\n**Explanation of Superior Performance:**\n\nThe significant outperformance of the ensemble models, particularly the Stacking Regressor, over the single baseline model can be explained by referencing the **bias-variance trade-off** and the principle of **model diversity**.\n\n1.  **Addressing the Bias-Variance Trade-off:**\n    *   The baseline Linear Regression model, while better than a simple Decision Tree, was a **high-bias** model. It was too simple (linear) to capture the complex, non-linear relationships between factors like the hour of the day, weather, and season on bike rental demand. Its high RMSE of 100.4449 reflects this fundamental inability to model the data's true complexity.\n    *   The Gradient Boosting model made its greatest leap in performance by directly attacking this problem. As a **bias-reduction** technique, it sequentially built models to correct the errors of its predecessors, creating a highly complex and accurate function that dramatically lowered the bias, resulting in a much lower RMSE (54.1999).\n    *   Stacking provided the final, optimal balance. It took the powerful, low-bias Gradient Boosting model and combined it with other models (like the lower-variance Bagging model). The meta-learner's role is to find the best possible compromise in the bias-variance trade-off, using the strengths of each base learner to compensate for the weaknesses of others.\n\n2.  **The Power of Model Diversity:**\n    *   The core strength of Stacking lies in its use of **diverse base learners**. We didn't just combine three similar models; we combined three different approaches to problem-solving:\n        *   **Gradient Boosting:** A sequential, error-correcting approach.\n        *   **Bagging:** A parallel approach that reduces variance by averaging.\n        *   **K-Nearest Neighbors:** An instance-based, non-parametric approach that makes predictions based on local data points.\n    *   Because these models have different underlying assumptions, they make different kinds of errors. The Stacking meta-learner is explicitly trained to learn these error patterns. It learns when to trust the Boosting model, when to lean more on the Bagging model's stability, and when the KNN model's local perspective is valuable. This \"wisdom of the diverse crowd\" allows it to make a final prediction that is more robust and accurate than any single \"expert\" could achieve on its own.\n\nIn summary, the Stacking Regressor won because it didn't rely on a single perspective. It intelligently synthesized the predictions from a diverse team of specialized models, creating a final, synergistic model that was far more powerful and accurate than the sum of its parts.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}